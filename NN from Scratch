import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from PIL import Image

# Define your neural network classes here...

class Layer_Dense:
    def __init__(self, n_inputs, n_neurons):
        self.weights = np.random.randn(n_inputs, n_neurons) * np.sqrt(2. / n_inputs)
        self.biases = np.zeros((1, n_neurons))
    
    def forward(self, inputs):
        self.inputs = inputs
        self.output = np.dot(inputs, self.weights) + self.biases
    
    def backward(self, dvalues):
        self.dweights = np.dot(self.inputs.T, dvalues)
        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)
        self.dinputs = np.dot(dvalues, self.weights.T)

class Activation_ReLU:
    def forward(self, inputs):
        self.inputs = inputs
        self.output = np.maximum(0, inputs)
    
    def backward(self, dvalues):
        self.dinputs = dvalues.copy()
        self.dinputs[self.inputs <= 0] = 0

class Activation_Softmax:
    def forward(self, inputs):
        self.inputs = inputs
        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))
        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)
        self.output = probabilities
        return probabilities 
    
    def backward(self, dvalues):
        self.dinputs = np.empty_like(dvalues)
        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):
            single_output = single_output.reshape(-1, 1)
            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)
            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)


class Dropout:
    def __init__(self, dropout_rate):
        self.dropout_rate = dropout_rate
    
    def forward(self, inputs, training=True):
        self.inputs = inputs
        if training:
            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=inputs.shape)
            self.output = inputs * self.mask
        else:
            self.output = inputs
    
    def backward(self, dvalues):
        self.dinputs = dvalues * self.mask

class Optimizer_Adam:
    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):
        self.learning_rate = learning_rate
        self.current_learning_rate = learning_rate
        self.decay = decay
        self.iterations = 0
        self.epsilon = epsilon
        self.beta_1 = beta_1
        self.beta_2 = beta_2
    
    def pre_update_params(self):
        if self.decay:
            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))
    
    def update_params(self, layer):
        if not hasattr(layer, 'weight_momentums'):
            layer.weight_momentums = np.zeros_like(layer.weights)
            layer.weight_cache = np.zeros_like(layer.weights)
            layer.bias_momentums = np.zeros_like(layer.biases)
            layer.bias_cache = np.zeros_like(layer.biases)
        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights
        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases
        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))
        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))
        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2
        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2
        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))
        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))
        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)
        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)
    
    def post_update_params(self):
        self.iterations += 1

class Loss:
    def calculate(self, output, y):
        sample_losses = self.forward(output, y)
        data_loss = np.mean(sample_losses)
        return data_loss

class Loss_CategoricalCrossentropy(Loss):
    def forward(self, y_pred, y_true):
        samples = len(y_pred)
        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)
        if len(y_true.shape) == 1:
            correct_confidences = y_pred_clipped[range(samples), y_true]
        elif len(y_true.shape) == 2:
            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)
        negative_log_likelihoods = -np.log(correct_confidences)
        return negative_log_likelihoods
    
    def backward(self, dvalues, y_true):
        samples = len(dvalues)
        if len(y_true.shape) == 2:
            y_true = np.argmax(y_true, axis=1)
        self.dinputs = dvalues.copy()
        self.dinputs[range(samples), y_true] -= 1
        self.dinputs = self.dinputs / samples

class Activation_Softmax_Loss_CategoricalCrossentropy:
    def __init__(self):
        self.activation = Activation_Softmax()
        self.loss = Loss_CategoricalCrossentropy()
    
    def forward(self, inputs, y_true):
        self.activation.forward(inputs)
        self.output = self.activation.output
        return self.loss.calculate(self.output, y_true)
    
    def backward(self, dvalues, y_true):
        # Apply gradient of categorical cross-entropy loss
        if len(y_true.shape) == 2:
            y_true = np.argmax(y_true, axis=1)
        
        # Create an array of gradients
        self.dinputs = dvalues.copy()
        
        # Subtract 1 from the true class indices
        self.dinputs[range(len(dvalues)), y_true] -= 1
        
        # Normalize the gradients
        self.dinputs = self.dinputs / len(dvalues)


class CustomNeuralNetwork:
    def __init__(self):
        # Define layers
        self.dense1 = Layer_Dense(28*28, 512)
        self.activation1 = Activation_ReLU()
        self.dropout1 = Dropout(0.2)
        
        self.dense2 = Layer_Dense(512, 256)
        self.activation2 = Activation_ReLU()
        self.dropout2 = Dropout(0.2)
        
        self.dense3 = Layer_Dense(256, 128)
        self.activation3 = Activation_ReLU()
        self.dropout3 = Dropout(0.2)
        
        self.dense4 = Layer_Dense(128, 64)
        self.activation4 = Activation_ReLU()
        self.dropout4 = Dropout(0.2)
        
        self.dense5 = Layer_Dense(64, 10)
        self.loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()
        self.optimizer = Optimizer_Adam(learning_rate=0.001, decay=1e-4)
    
    def forward(self, X, training=True):
        self.dense1.forward(X)
        self.activation1.forward(self.dense1.output)
        self.dropout1.forward(self.activation1.output, training)
        
        self.dense2.forward(self.dropout1.output)
        self.activation2.forward(self.dense2.output)
        self.dropout2.forward(self.activation2.output, training)
        
        self.dense3.forward(self.dropout2.output)
        self.activation3.forward(self.dense3.output)
        self.dropout3.forward(self.activation3.output, training)
        
        self.dense4.forward(self.dropout3.output)
        self.activation4.forward(self.dense4.output)
        self.dropout4.forward(self.activation4.output, training)
        
        self.dense5.forward(self.dropout4.output)
        return self.dense5.output
    
    def predict(self, X):
        output = self.forward(X, training=False)
        return output

def preprocess_image(img_path):
    # Load the image
    img = Image.open(img_path)

    # Resize the image to 28x28
    img = img.resize((28, 28))

    # Convert the image to grayscale
    img = img.convert('L')

    # Convert the image to a numpy array
    img_array = np.array(img)

    # Normalize the pixel values to be between 0 and 1
    img_array = img_array / 255.0

    # Invert the colors
    img_array = 1 - img_array

    # Flatten the image array
    img_array = img_array.flatten()

    # Add a batch dimension to the image array
    img_array = np.expand_dims(img_array, axis=0)

    return img_array

# Load MNIST data and train the model
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28*28) / 255.0
x_test = x_test.reshape(-1, 28*28) / 255.0
y_train_one_hot = np.eye(10)[y_train]
y_test_one_hot = np.eye(10)[y_test]

model = CustomNeuralNetwork()

batch_size = 25
n_batches = x_train.shape[0] // batch_size

for epoch in range(5):  # Reduced number of epochs to 5
    epoch_loss = 0
    epoch_accuracy = 0
    
    for batch_start in range(0, x_train.shape[0], batch_size):
        batch_end = min(batch_start + batch_size, x_train.shape[0])
        X_batch = x_train[batch_start:batch_end]
        y_batch = y_train_one_hot[batch_start:batch_end]
        
        # Forward pass
        predictions = model.forward(X_batch)
        loss = model.loss_activation.forward(predictions, y_batch)
        
        # Calculate accuracy
        predictions_class = np.argmax(predictions, axis=1)
        accuracy = np.mean(predictions_class == y_train[batch_start:batch_end])
        epoch_accuracy += accuracy * (batch_end - batch_start)
        epoch_loss += np.sum(loss)
        
        # Backward pass
        model.loss_activation.backward(predictions, y_batch)
        model.dense5.backward(model.loss_activation.dinputs)
        model.dropout4.backward(model.dense5.dinputs)
        model.activation4.backward(model.dropout4.dinputs)
        model.dense4.backward(model.activation4.dinputs)
        model.dropout3.backward(model.dense4.dinputs)
        model.activation3.backward(model.dropout3.dinputs)
        model.dense3.backward(model.activation3.dinputs)
        model.dropout2.backward(model.dense3.dinputs)
        model.activation2.backward(model.dropout2.dinputs)
        model.dense2.backward(model.activation2.dinputs)
        model.dropout1.backward(model.dense2.dinputs)
        model.activation1.backward(model.dropout1.dinputs)
        model.dense1.backward(model.activation1.dinputs)
        
        # Update parameters
        model.optimizer.pre_update_params()
        model.optimizer.update_params(model.dense1)
        model.optimizer.update_params(model.dense2)
        model.optimizer.update_params(model.dense3)
        model.optimizer.update_params(model.dense4)
        model.optimizer.update_params(model.dense5)
        model.optimizer.post_update_params()
    
    epoch_loss /= x_train.shape[0]
    epoch_accuracy /= x_train.shape[0]
    
    print(f'Epoch: {epoch + 1}, ' +
          f'Acc: {epoch_accuracy:.3f}, ' +
          f'Loss: {epoch_loss:.3f}, ' +
          f'LR: {model.optimizer.current_learning_rate}')

# Predicting
img_path = r'D:\n_00.png'
preprocessed_img = preprocess_image(img_path)
predictions = model.predict(preprocessed_img)
probabilities = Activation_Softmax().forward(predictions)
predicted_class = np.argmax(probabilities)

print(f'The predicted digit is: {predicted_class}')

img_path = r'D:\n_11.png'
preprocessed_img = preprocess_image(img_path)
predictions = model.predict(preprocessed_img)
probabilities = Activation_Softmax().forward(predictions)
predicted_class = np.argmax(probabilities)

print(f'The predicted digit is: {predicted_class}')





